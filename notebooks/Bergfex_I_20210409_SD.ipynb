{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bergfex Webscraping\n",
    "<b> Milestone 1</b> \n",
    "\n",
    "Scraping the Bergfex.com website in order to create Pandas dataframes that contain important information about each trail. This code uses BeautifulSoup to parse the html tags into json, which is easier to work with. A for-loop iterates over each html tag and adds the corresponding information into empty lists. Data is cleaned and an initial data analysis is performed.\n",
    "Finally, also the corresponding GPS data is downloaded and scraped.\n",
    "The functions created in this notebook allow to easily repeat the steps or perform them for different regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unhash and run the below line once\n",
    "#!pip install lxml\n",
    "#!pip install wget\n",
    "#!pip install gpxpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4ba224276af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlxml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "import lxml\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import glob\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import gpxpy \n",
    "import gpxpy.gpx \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Still to do:\n",
    "- Add key words: Webscraping from a static page, Data Cleaning, plots, GPX coordinates, KML coordinates (add both? describe the differences?), spatial data, tableau, visualization, seaborn, matplotlib\n",
    "\n",
    "gpx-files geospatial-data weather-data webscraping data-cleaning hiking-trails snow-level\n",
    "simple-stats visualization seaborn matplotlib\n",
    "\n",
    "- DONE: clean up of functions\n",
    "- DONE: clean up gpx all df: name, trail_id\n",
    "- DONE: check if example ID is in DF for google earth\n",
    "- DONE: test final functions\n",
    "- DONE: download of final GPX files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1  Data scraping\n",
    "\n",
    "Prepare to scrape the Bergfex.com website in order to create Pandas dataframes that contain important information about each trail. This code uses BeautifulSoup to parse the html tags into json. A for-loop iterates over each html tag and adds the corresponding information into empty lists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Scrape all the information (basic and more complicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []  # Title of the tour\n",
    "difficulty = []  # difficulty (easy, medium, hard)\n",
    "sport = []  # Sport type (hiking, sledging, snowshoe...)\n",
    "length = []  # length in km\n",
    "time_list = []  # tour time in hours:minutes\n",
    "climb = []  # positive elevation climb in m\n",
    "minmax = []  # minimum and maximum altitude of the tour in m\n",
    "technique = []  # technique difficulty ratings (out of 6)\n",
    "fitness = []  # fitness difficulty ratings (out of 6)\n",
    "total_title = [] # will help extracting the ID of the tour\n",
    "\n",
    "# all ratings are stored together so we will need this along the way:\n",
    "rating = []  # list to store technique and fitness rating info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping over the x first pages of Bergfex\n",
    "# each separate tour on the page is framed by a div tag with 'touren-details'\n",
    "\n",
    "page_number = 2  # number of pages we want to scrape through\n",
    "\n",
    "for p in range(1, (page_number+1)):\n",
    "    base_link = 'https://www.bergfex.com/sommer/bern-region/touren/?isAjax=1&page='\n",
    "    link = base_link+str(p)  # going over p pages with numbers appended to the base link\n",
    "    page = requests.get(link, timeout=5)\n",
    "    print(\"scraped page\", p)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")  # bs4.BeautifulSoup object\n",
    "    tours = soup.findAll('div', {'class': 'touren-detail'})  # checks for the separate tours on the page\n",
    "\n",
    "# For each page, we loop over each tour and fill our lists with info     \n",
    "    for i in range(0,len(tours)):\n",
    "        tour_1 = tours[i] # tour iterating over the 20 tours of the page\n",
    "\n",
    "        tour_title = tour_1.findAll('a' ) # this gets the title out\n",
    "        title.append([info.get_text().strip() for info in tour_title])\n",
    "        total_title.append(tour_title) \n",
    "        # this gets the full tour-title information including the ID, will need to be processed further later on\n",
    "        \n",
    "        tour_diff = tour_1.findAll('span', {'class': 'tour-difficulty'}) #tour level difficulty\n",
    "        difficulty.append([info.get_text().strip() for info in tour_diff])\n",
    "\n",
    "        tour_type = tour_1.findAll('span', {'class': 'tour-type'}) # putting type into sports\n",
    "        sport.append([info.get_text().strip() for info in tour_type])\n",
    "\n",
    "        tour_stats = tour_1.findAll('div', {'class': 'tour-stats'}) # stats has 4 info binned together\n",
    "        stat_text = [info.get_text().strip() for info in tour_stats]\n",
    "        length.append(stat_text[0])\n",
    "        time_list.append(stat_text[1])\n",
    "        climb.append(stat_text[2])\n",
    "        minmax.append(stat_text[3])\n",
    "        \n",
    "        tour_rating = tour_1.find_all(\"div\", {'class': 'tour-rating'}) # getting the rating data \n",
    "        rating.append([info for info in tour_rating])   # it's a class name so we can't get_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of tours collected:\", len(title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Extracting embedded HTML tags: <a href=\"\" \n",
    "Each trail name and ID was used in Bergfex's URL. Using the trail ID in the dataframe will provide a unique ID to use later in the analysis. However, those two data points are embedded deeper in the html code. The below codes shows how to extract the datapoints into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting the ID via the URL\n",
    "links_title = []\n",
    "\n",
    "for i in total_title:\n",
    "    i = str(i)\n",
    "    i = i.strip('[]')\n",
    "    links_title.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the trail ID usable it needs to be cleaned up and converted into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting ID\n",
    "id_df3 = pd.DataFrame(links_title)\n",
    "id_df2 = id_df3[0].str.split(' ',expand = True)\n",
    "id_df1 = id_df2[2].str.split(',',expand = True)\n",
    "id_df = id_df1[0].str.split('/',expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this format:\n",
    "pd.options.display.max_colwidth = 100\n",
    "id_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to this format:\n",
    "id_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Extracting embedded HTML tags: <Div class=\"\"\n",
    "The HTML tags for the fitness and technique ratings were embedded within Div class tags; they are extracted using a for-loop and then stripped of unecessary characeters. \n",
    "- Convert the tour rating tag into a string\n",
    "- Strip away unecessary characters \n",
    "- Split into lists which look like rating_list [Technique, Fitness]\n",
    "- Finally, convert into seperate lists for Technique and Fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating[0] # View the emmbedded DIV tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace unwanted information\n",
    "tour_rating_str = str(rating)\n",
    "rating_all_short = tour_rating_str.replace(\n",
    "'<div class=\"tour-rating\">\\n<div class=\"tour-rating-label\">Technique</div>\\n<div class=\"rating-circles rating-max6\"><div class=\"',''\n",
    ").replace(\n",
    "    '<div class=\"tour-rating\">\\n<div class=\"tour-rating-label\">Fitness</div>\\n<div class=\"rating-circles rating-max6\"><div class=\"','').replace('\"></div></div>\\n</div>',\"\")\n",
    "rating_even_shorter = rating_all_short.replace('[','').replace(']','')\n",
    "rating_list = rating_even_shorter.split(\", \")\n",
    "\n",
    "for i in range(0,(len(rating_list))):\n",
    "    if i == 0:\n",
    "        technique.append(rating_list[i]) # add first item in technique\n",
    "    elif i % 2 == 0:\n",
    "        technique.append(rating_list[i]) # then every second item as well\n",
    "    else:\n",
    "        fitness.append(rating_list[i]) # the other appended to fitness list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Let's look at the result\n",
    "\n",
    "#rating_all_short\n",
    "#rating_even_shorter\n",
    "rating_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4  Create dataframe for extracted elements\n",
    "\n",
    "This dataframe is not in the final stage yet as there are still extra brackets, the minimum and maximum elevation are still in one column, and units must be stripped away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df_unclean = pd.DataFrame(\n",
    "    {'title': title,\n",
    "     'difficulty': difficulty,\n",
    "     'sport': sport,\n",
    "     'length': length,\n",
    "     'time': time_list,\n",
    "     'climb': climb,\n",
    "     'minmax': minmax,\n",
    "     'technique': technique,\n",
    "     'fitness': fitness\n",
    "    })\n",
    "activities_df_unclean['ID']= id_df.iloc[:,5].copy() # add the ID columns from an earlier DF\n",
    "activities_df_unclean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Write CSV file of scrapped and uncleaned DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df_unclean.to_csv(\"../data/unclean_activities.csv\", index=False)\n",
    "activities_df_unclean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.6 Combine all in scraping function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to repeat the same steps for a different region (for example \"Wallis\"), we put the scraping code into a function. Here we can choose the region, how many pages we want to scrape and whether we want to save the file as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_region(region, n_pages):\n",
    "    title = []  # Title of the tour\n",
    "    difficulty = []  # difficulty (easy, medium, hard)\n",
    "    sport = []  # Sport type (hiking, sledging, snowshoe...)\n",
    "    length = []  # length in km\n",
    "    time_list = []  # tour time in hours:minutes\n",
    "    climb = []  # positive elevation climb in m\n",
    "    minmax = []  # minimum and maximum altitude of the tour in m\n",
    "    total_title = [] # will help extracting the ID of the tour\n",
    "    # all ratings are stored together so we will need this along the way:\n",
    "    rating = []  # list to store technique and fitness rating info\n",
    "    \n",
    "    # looping over the x first pages of Bergfex\n",
    "    # each separate tour on the page is framed by a div tag with 'touren-details'\n",
    "    page_number = n_pages  # number of pages we want to scap through\n",
    "\n",
    "    for p in range(1, (page_number+1)):\n",
    "        base_link = f'https://www.bergfex.com/sommer/{region}/touren/?isAjax=1&page='\n",
    "        link = base_link+str(p)  # going over p pages with numbers appended to the base link\n",
    "        page = requests.get(link, timeout=5)\n",
    "        print(\"scraped page\", p)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")  # bs4.BeautifulSoup object\n",
    "        tours = soup.findAll('div', {'class': 'touren-detail'})  # checks for the separate tours on the page\n",
    "\n",
    "    # For each page, we loop over each tour and fill our lists with info     \n",
    "        for i in range(0,len(tours)):\n",
    "            tour_1 = tours[i] # tour iterating over the 20 tours of the page\n",
    "\n",
    "            tour_title = tour_1.findAll('a' ) # this gets the title out\n",
    "            title.append([info.get_text().strip() for info in tour_title])\n",
    "            total_title.append(tour_title) \n",
    "            # this gets the full tour-title information including the ID, processed further later on\n",
    "\n",
    "            tour_diff = tour_1.findAll('span', {'class': 'tour-difficulty'}) #tour level difficulty\n",
    "            difficulty.append([info.get_text().strip() for info in tour_diff])\n",
    "\n",
    "            tour_type = tour_1.findAll('span', {'class': 'tour-type'}) # putting type into sports\n",
    "            sport.append([info.get_text().strip() for info in tour_type])\n",
    "\n",
    "            tour_stats = tour_1.findAll('div', {'class': 'tour-stats'}) # stats has 4 info binned together\n",
    "            stat_text = [info.get_text().strip() for info in tour_stats]\n",
    "            length.append(stat_text[0])\n",
    "            time_list.append(stat_text[1])\n",
    "            climb.append(stat_text[2])\n",
    "            minmax.append(stat_text[3])\n",
    "\n",
    "            tour_rating = tour_1.find_all(\"div\", {'class': 'tour-rating'}) # getting the rating data \n",
    "            rating.append([info for info in tour_rating])   # it's a class name so we can't get_text.\n",
    "    print(\"number of tours collected:\", len(title))\n",
    "    return (title, difficulty, sport, length, time_list, climb, minmax, total_title, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_id(total_title):\n",
    "    links_title = []\n",
    "    for i in total_title:\n",
    "        i = str(i)\n",
    "        i = i.strip('[]')\n",
    "        links_title.append(i)\n",
    "    # Extracting ID\n",
    "    id_df3 = pd.DataFrame(links_title)\n",
    "    id_df2 = id_df3[0].str.split(' ',expand = True)\n",
    "    id_df1 = id_df2[2].str.split(',',expand = True)\n",
    "    id_df = id_df1[0].str.split('/',expand = True)\n",
    "    \n",
    "    return id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_rating(rating):\n",
    "    technique = []  # technique difficulty ratings (out of 6)\n",
    "    fitness = []  # fitness difficulty ratings (out of 6)\n",
    "    # Replace unwanted information\n",
    "    tour_rating_str = str(rating)\n",
    "    rating_all_short = tour_rating_str.replace(\n",
    "    '<div class=\"tour-rating\">\\n<div class=\"tour-rating-label\">Technique</div>\\n<div class=\"rating-circles rating-max6\"><div class=\"',''\n",
    "    ).replace(\n",
    "        '<div class=\"tour-rating\">\\n<div class=\"tour-rating-label\">Fitness</div>\\n<div class=\"rating-circles rating-max6\"><div class=\"','').replace('\"></div></div>\\n</div>',\"\")\n",
    "    rating_even_shorter = rating_all_short.replace('[','').replace(']','')\n",
    "    rating_list = rating_even_shorter.split(\", \")\n",
    "\n",
    "    for i in range(0,(len(rating_list))):\n",
    "        if i == 0:\n",
    "            technique.append(rating_list[i]) # add first item in technique\n",
    "        elif i % 2 == 0:\n",
    "            technique.append(rating_list[i]) # then every second item as well\n",
    "        else:\n",
    "            fitness.append(rating_list[i]) # the other appended to fitness list\n",
    "    return technique, fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scraped_df(region, n_pages, save_as_csv = True):\n",
    "    title, difficulty, sport, length, time_list, climb, minmax, total_title, rating = scraping_region(\n",
    "        region, n_pages)\n",
    "    id_df = extracting_id(total_title)\n",
    "    technique, fitness = extracting_rating(rating)\n",
    "    activities_df_unclean = pd.DataFrame(\n",
    "    {'title': title,\n",
    "     'difficulty': difficulty,\n",
    "     'sport': sport,\n",
    "     'length': length,\n",
    "     'time': time_list,\n",
    "     'climb': climb,\n",
    "     'minmax': minmax,\n",
    "     'technique': technique,\n",
    "     'fitness': fitness\n",
    "    })\n",
    "    activities_df_unclean['ID']= id_df.iloc[:,5].copy() # add the ID columns from an earlier DF\n",
    "    \n",
    "    if save_as_csv == True:\n",
    "        activities_df_unclean.to_csv(f\"../data/unclean_activities_{region}.csv\", index=False)\n",
    "        \n",
    "    return activities_df_unclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with different regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region2 = \"wallis\"\n",
    "n_pages2 = 20\n",
    "#create_scraped_df(region2, n_pages2, save_as_csv = True)\n",
    "#create_scraped_df('bern-region', n_pages2, save_as_csv = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2  Data Cleaning\n",
    "\n",
    "As mentioned above, the dataframe still needs to be cleaned up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Read in the \"uncleaned\" DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df_raw = pd.read_csv(f\"../data/unclean_activities.csv\")\n",
    "activities_df = activities_df_raw.copy()\n",
    "activities_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Check for duplicates and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if there are duplicates\n",
    "dups = activities_df.duplicated(subset=['ID'])\n",
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which trails are duplicates?\n",
    "try:\n",
    "    pd.concat(g for _, g in activities_df.groupby('ID') if len(g) > 1)\n",
    "except ValueError:\n",
    "    print('no duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "activities_df.drop_duplicates(subset =\"ID\", inplace = True)\n",
    "activities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Clean Categorical Data\n",
    "- Convert columns to strings\n",
    "- Remove brackets that are in the first 3 columnns\n",
    "- Remove extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the text columns to strings for easier handling later\n",
    "activities_df[['title', 'difficulty', 'sport']] = activities_df[['title', 'difficulty', 'sport']].astype('str')\n",
    "\n",
    "# Removing the brackets\n",
    "activities_df['title'] = activities_df['title'].str.strip('[]')\n",
    "activities_df['difficulty'] = activities_df['difficulty'].str.strip('[]')\n",
    "activities_df['sport'] = activities_df['sport'].str.strip('[]')\n",
    "\n",
    "# Remove extra whitespace\n",
    "activities_df['title'] = pd.Series(activities_df['title']).str.replace(\"'\", '')\n",
    "activities_df['difficulty'] = pd.Series(activities_df['difficulty']).str.replace(\"'\", '')\n",
    "activities_df['sport'] = pd.Series(activities_df['sport']).str.replace(\"'\", '')\n",
    "\n",
    "# Remove units (km, m, h, hm, m)\n",
    "activities_df['length'] = pd.Series(activities_df['length']).str.replace(\"km\", '')\n",
    "activities_df['time'] = pd.Series(activities_df['time']).str.replace(\"h\", '')\n",
    "activities_df['climb'] = pd.Series(activities_df['climb']).str.replace(\"hm\", '')\n",
    "activities_df['minmax'] = pd.Series(activities_df['minmax']).str.replace(\"m\", '')\n",
    "\n",
    "# Seperate the minmax altitude into a min column and a max column\n",
    "activities_df[['min','max']] = activities_df['minmax'].str.split(\"-\",expand=True)\n",
    "activities_df = activities_df.drop(columns=['minmax'])\n",
    "\n",
    "# Organize the dataframe columns\n",
    "activities_df = activities_df[['ID','title', 'difficulty', 'sport', 'length', \n",
    "                               'time', 'climb', 'min', 'max', 'technique', 'fitness']]\n",
    "\n",
    "# Remove remaining commas to avoid issues with reading CSV file in Tableau\n",
    "activities_df['title'] = pd.Series(activities_df['title']).str.replace(\",\", '-')\n",
    "\n",
    "# Add ordering of the difficulty\n",
    "activities_df['difficulty'] = pd.Series(activities_df['difficulty']).str.replace(\"easy\", '1 - easy')\n",
    "activities_df['difficulty'] = pd.Series(activities_df['difficulty']).str.replace(\"medium\", '2 - medium')\n",
    "activities_df['difficulty'] = pd.Series(activities_df['difficulty']).str.replace(\"difficult\", '3 - difficult')\n",
    "activities_df['difficulty'].replace(r'^\\s*$', '0 - no rating', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Clean Numerical Data\n",
    "\n",
    "In order to normalize numerical data for analysis, we must preform the following steps:\n",
    "\n",
    "-  Replace missing numerical data with 0\n",
    "-  Remove commas\n",
    "-  Remove \"-\"\n",
    "-  Remove unnecessary words (\"rating-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df['length'] = pd.Series(activities_df['length']).str.replace(\"-\",'0')\n",
    "activities_df['length'] = (activities_df['length']).astype(float)\n",
    "\n",
    "activities_df['climb'] = pd.Series(activities_df['climb']).str.replace(\",\", '') \n",
    "activities_df['climb'] = pd.Series(activities_df['climb']).str.replace(\"-\", '0') \n",
    "activities_df['climb'] = (activities_df['climb']).astype(int)\n",
    "\n",
    "activities_df['min'] = pd.Series(activities_df['min']).str.replace(\",\", '')\n",
    "activities_df['min'].replace('',np.nan, inplace = True)\n",
    "activities_df['min'] = activities_df['min'].fillna(0)\n",
    "activities_df['min'] = (activities_df['min']).astype(int)\n",
    "activities_df['max'].replace('',np.nan, inplace = True)\n",
    "activities_df['max'] = activities_df['max'].fillna(0)\n",
    "activities_df['max'] = pd.Series(activities_df['max']).str.replace(\",\", '') \n",
    "\n",
    "activities_df['max'] = (activities_df['max']).astype(int)\n",
    "\n",
    "activities_df['technique'] = pd.Series(activities_df['technique']).str.replace(\"rating-\", '')\n",
    "activities_df['technique'] = pd.to_numeric(activities_df['technique'], errors='coerce')\n",
    "activities_df['technique'] = pd.Series(activities_df['technique']).replace(np.nan, 0, regex = True)\n",
    "activities_df['technique'] = pd.Series(activities_df['technique']).astype('int')\n",
    "\n",
    "activities_df['fitness'] = pd.Series(activities_df['fitness']).str.replace(\"rating-\", '')\n",
    "activities_df['fitness'] = pd.to_numeric(activities_df['fitness'], errors='coerce')\n",
    "activities_df['fitness'] = pd.Series(activities_df['fitness']).replace(np.nan, 0, regex = True)\n",
    "activities_df['fitness'] = activities_df['fitness'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Final check of DataFrame\n",
    "Double check for null values. It looks pretty good!  Now it's time to start analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activities_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Write CSV\n",
    "\n",
    "Now that the data has been cleaned properly, let's save the dataframe to a CSV for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df.to_csv(\"../data/activities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Functions for Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again save these codes as functions to be able to run the script easily for other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cat(activities_df):\n",
    "    # Converting the text columns to strings for easier handling later\n",
    "    activities_df[['title', 'difficulty', 'sport']] = activities_df[['title', 'difficulty', 'sport']].astype('str')\n",
    "\n",
    "    # Removing the brackets\n",
    "    activities_df['title'] = activities_df['title'].str.strip('[]')\n",
    "    activities_df['difficulty'] = activities_df['difficulty'].str.strip('[]')\n",
    "    activities_df['sport'] = activities_df['sport'].str.strip('[]')\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    activities_df['title'] = pd.Series(activities_df['title']).str.replace(\"'\", '')\n",
    "    activities_df['difficulty'] = pd.Series(activities_df['difficulty']).str.replace(\"'\", '')\n",
    "    activities_df['sport'] = pd.Series(activities_df['sport']).str.replace(\"'\", '')\n",
    "\n",
    "    # Remove units (km, m, h, hm, m)\n",
    "    activities_df['length'] = pd.Series(activities_df['length']).str.replace(\"km\", '')\n",
    "    activities_df['time'] = pd.Series(activities_df['time']).str.replace(\"h\", '')\n",
    "    activities_df['climb'] = pd.Series(activities_df['climb']).str.replace(\"hm\", '')\n",
    "    activities_df['minmax'] = pd.Series(activities_df['minmax']).str.replace(\"m\", '')\n",
    "\n",
    "    # Seperate the minmax altitude into a min column and a max column\n",
    "    activities_df[['min','max']] = activities_df['minmax'].str.split(\"-\",expand=True)\n",
    "    activities_df = activities_df.drop(columns=['minmax'])\n",
    "\n",
    "    # Organize the dataframe columns\n",
    "    activities_df = activities_df[['ID','title', 'difficulty', 'sport', 'length', \n",
    "                                   'time', 'climb', 'min', 'max', 'technique', 'fitness']]\n",
    "\n",
    "    # Add ordering of the difficulty\n",
    "    activities_df['difficulty'] = pd.Series(activities_df['difficulty']).str.replace(\"easy\", '1 - easy')\n",
    "    activities_df['difficulty'] = pd.Series(activities_df['difficulty']).str.replace(\"medium\", '2 - medium')\n",
    "    activities_df['difficulty'] = pd.Series(activities_df['difficulty']).str.replace(\"difficult\",'3 - difficult')\n",
    "    activities_df['difficulty'].replace(r'^\\s*$', '0 - no rating', regex=True, inplace=True)\n",
    "    return activities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_num(activities_df):\n",
    "    activities_df['length'] = pd.Series(activities_df['length']).str.replace(\"-\",'0')\n",
    "    activities_df['length'] = (activities_df['length']).astype(float)\n",
    "\n",
    "    activities_df['climb'] = pd.Series(activities_df['climb']).str.replace(\",\", '') \n",
    "    activities_df['climb'] = pd.Series(activities_df['climb']).str.replace(\"-\", '0') \n",
    "    activities_df['climb'] = (activities_df['climb']).astype(int)\n",
    "    \n",
    "    activities_df['min'] = pd.Series(activities_df['min']).str.replace(\",\", '')\n",
    "    activities_df['min'].replace('',np.nan, inplace = True)\n",
    "    activities_df['min'] = activities_df['min'].fillna(0)\n",
    "    activities_df['min'] = (activities_df['min']).astype(int)\n",
    "    activities_df['max'].replace('',np.nan, inplace = True)\n",
    "    activities_df['max'] = activities_df['max'].fillna(0)\n",
    "    activities_df['max'] = pd.Series(activities_df['max']).str.replace(\",\", '') \n",
    "\n",
    "    activities_df['technique'] = pd.Series(activities_df['technique']).str.replace(\"rating-\", '')\n",
    "    activities_df['technique'] = pd.to_numeric(activities_df['technique'], errors='coerce')\n",
    "    activities_df['technique'] = pd.Series(activities_df['technique']).replace(np.nan, 0, regex = True)\n",
    "    activities_df['technique'] = pd.Series(activities_df['technique']).astype('int')\n",
    "\n",
    "    activities_df['fitness'] = pd.Series(activities_df['fitness']).str.replace(\"rating-\", '')\n",
    "    activities_df['fitness'] = pd.to_numeric(activities_df['fitness'], errors='coerce')\n",
    "    activities_df['fitness'] = pd.Series(activities_df['fitness']).replace(np.nan, 0, regex = True)\n",
    "    activities_df['fitness'] = activities_df['fitness'].astype('int')\n",
    "    return activities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_time(activities_df):\n",
    "    try:\n",
    "        activities_df['time'] = activities_df['time'].dt.strftime('%H:%M')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return activities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(region, save_as_csv = True):\n",
    "    activities_df_unclean = pd.read_csv(f\"../data/unclean_activities_{region}.csv\")\n",
    "    activities_df_unclean.drop_duplicates(subset =\"ID\", inplace = True)\n",
    "    activities_df = clean_cat(activities_df_unclean)\n",
    "    activities_df = clean_num(activities_df)\n",
    "    #clean_time(activities_df)\n",
    "    \n",
    "    if save_as_csv == True:\n",
    "        activities_df.to_csv(f\"../data/activities_{region}.csv\", index=False)\n",
    "        \n",
    "    return activities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(region, n_pages, save_as_csv = True):\n",
    "    if save_as_csv == True:\n",
    "        save = True\n",
    "    else:\n",
    "        save = False\n",
    "    activities_df_unclean = create_scraped_df(region, n_pages, save_as_csv = save)\n",
    "    activities_df = clean_df(region, save_as_csv = save)\n",
    "        \n",
    "    return activities_df, activities_df_unclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the code for a new region or for all regions through a for-loop.\n",
    "Afterwards, we hash out the code. Otherwise if you rerun the Kernel too often, there is a risk of being blocked by the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_test1 = 'bern-region'\n",
    "n_pages_test = 35\n",
    "#result_test1 = preprocess(region_test1, n_pages_test, save_as_csv = True)\n",
    "\n",
    "regions = ['aargau', 'basel-region', 'bern-region', 'berneroberland', 'freiburg-region', \n",
    "           'genferseegebiet-waadtland', 'graubuenden', 'jura-drei-seen-land', 'luzern-vierwaldstaettersee',\n",
    "          'ostschweiz', 'tessin', 'wallis', 'zuerich']\n",
    "\n",
    "#for reg in regions:\n",
    " #   result_test_reg = preprocess(reg, n_pages_test, save_as_csv = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the CSV region files into one DF & CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_act_df = pd.DataFrame()\n",
    "for reg in regions:\n",
    "    activities_df_reg = pd.read_csv(f\"../data/activities_{reg}.csv\")\n",
    "    activities_df_reg['region'] = reg\n",
    "    all_act_df = pd.concat([all_act_df, activities_df_reg], axis = 0)\n",
    "    \n",
    "all_act_df.to_csv(f\"../data/activities_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_act_df['title'] = pd.Series(all_act_df['title']).str.replace(',', '-')\n",
    "all_act_df.to_csv(f\"../data/activities_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3  Initial Data Analysis\n",
    "\n",
    "Before moving onto combining weather data with hiking trails, lets do some preliminary data analysis to see what we are working with. For that, we use the complete \"bern-region\" data we just scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-dark-palette')\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "activities_df_raw_br = pd.read_csv(f\"../data/activities_bern-region.csv\")\n",
    "activities_df_br = activities_df_raw_br.copy()\n",
    "activities_df_br.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Exploration \n",
    "\n",
    "We will begin the data analysis by data exploration. By creating different data frames and removing activities with little data to improve visualizations we can get a better idea of what types of trails we are working with. \n",
    "\n",
    "Here we can see that cycling, racing, and long-distance walking are much longer than the other types of trails. Another helpful tool this DataFrame provides is that we can now view the least used trail types in the dataset. These are going to be removed for the next few analysis steps so it's easier to look at the interesting trail statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a df for \"Number of entries by sport type\" + average tour length\n",
    "sporttype = activities_df_br.groupby('sport') \\\n",
    "       .agg({'sport':'count', 'length':'mean'}) \\\n",
    "       .rename(columns={'sport':'count','length':'average_length'}) \\\n",
    "       .reset_index() \\\n",
    "       .sort_values(by=['average_length'])\n",
    "sporttype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Focus on the most relevant activities\n",
    "Remove the activities, which have less than 5 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = activities_df_br.groupby(\"sport\").filter(lambda x: len(x) > 10)\n",
    "analysis.groupby(['sport']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Bar Chart Visualization\n",
    "Now that the activities with little entries are removed, a barchart of the sport types is much more readable. We can see that the most popular activity on our list of trails is hiking with more than 200 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = analysis['sport'].value_counts().plot(kind='bar')\n",
    "plt.title('Frequency of Activity types')\n",
    "plt.xlabel('Type of Activity')\n",
    "plt.ylabel('Count of Activity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the picture look like by level of difficulty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-078c1eb7ef62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manalysis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'difficulty'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of Activities by Difficulty'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Difficulty'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Count of Activity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analysis' is not defined"
     ]
    }
   ],
   "source": [
    "analysis['difficulty'].value_counts().plot(kind='bar')\n",
    "plt.title('Number of Activities by Difficulty')\n",
    "plt.xlabel('Difficulty')\n",
    "plt.ylabel('Count of Activity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Scatter Plot Visualization\n",
    "\n",
    "The scatter plot paints an interesting picture of the trail data. First, the cycling trails are both the longest and the ones with most height difference out of the bunch. And second, it looks like all of the hiking trails fall between 2 and 20km (except for one). Perhaps this is because long distance hiking trails are considered to be \"Pilgrimages\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(    \n",
    "    data=analysis,\n",
    "    x=\"length\", y=\"climb\", hue='sport', \n",
    "    marker = \"D\", s=200, # set marker type and size\n",
    ").set_title('Length and Distance By Sport Type')\n",
    "plt.legend(loc='lower right'); # change default position of legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean length for difficulty:\n",
    "activities_df_br.groupby('difficulty').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore the data using only hiking trail data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Analysis: Hiking Trails \n",
    "\n",
    "The primary interest of this analysis is hiking trail data. The following analysis will compare only hiking trails and drop the rest of the sport types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiking = activities_df_br[activities_df_br.sport == \"Hiking\"]\n",
    "hiking.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Scatter plot: Length and Distance of each Activity By Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers1 = {\"1 - easy\": \".\", \"2 - medium\": \"p\",\n",
    "           \"3 - difficult\":\"s\", \"0 - no rating\":\"X\"} #set the markers individually\n",
    "sns.scatterplot(    \n",
    "    data=hiking,\n",
    "    x=\"length\", y=\"climb\", \n",
    "    hue='difficulty', # color by difficulty\n",
    "    hue_order = [\"1 - easy\", \"2 - medium\", \"3 - difficult\", \"0 - no rating\"], # rearrange order of the legend\n",
    "    style='difficulty', markers = markers1, # set markers\n",
    "    s = 400 # size of markers\n",
    ").set_title('Length and Distance By Difficulty');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems we have one \"outlier\" which is more than 200km long. Let's remove it to get a better view on the majority of the trails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(    \n",
    "    data=hiking[hiking.length < 200],\n",
    "    x=\"length\", y=\"climb\", \n",
    "    hue='difficulty', # color by difficulty\n",
    "    hue_order = [\"1 - easy\", \"2 - medium\", \"3 - difficult\", \"0 - no rating\"], # rearrange order of the legend\n",
    "    style='difficulty', markers = markers1, # set markers\n",
    "    s = 400, # size of markers\n",
    ").set_title('Length and Distance By Difficulty');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easier trails seem to be shorter in length and with lower elevation gain. Let's see if there is a correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Line Plot: Comparison of average Trail Length vs. Fitness Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataframe uses the fitness scores and compares them to the length of each trail. How correlated is the trail length to the fitness score? A new columns is added for average length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the count by Fitness level and the average length \n",
    "fitness_length_check = hiking.groupby('fitness') \\\n",
    "       .agg({'fitness':'count', 'length':'mean'}) \\\n",
    "       .rename(columns={'fitness':'count','length':'average_length'}) \\\n",
    "       .reset_index()\n",
    "fitness_length_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The routes with no fitness rating seems quite long. Probably related to the outlier previously identified.\n",
    "For further analysis let's remove the trails without any difficulty rating and check if there is a visual clue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_length = fitness_length_check[fitness_length_check['fitness'] != 0]\n",
    "plt.plot(fitness_length[\"fitness\"], fitness_length[\"average_length\"])\n",
    "plt.ylabel('average_length')\n",
    "plt.xlabel(\"rating_fitness\")\n",
    "plt.title('Hiking: Average Length by Fitness Level Rating')\n",
    "#sns.linewidth = 6\n",
    "#check linestyle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a relationship indeed between fitness score and length of the hike but let's have a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Boxplots: Difficulty vs. Elevation Gain and Length\n",
    "\n",
    "The following boxplots show how the trail data is distributed by difficulty catagory compared to both elevation gain and length. It's pretty clear that the relationship between elevation gain and difficulty is more meaningful than the relationship between distance and difficulty. My legs agree! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pal = {\"1 - easy\": \"forestgreen\", \"2 - medium\": \"darkblue\", \"3 - difficult\":\"darkred\", \n",
    "          \"0 - no rating\":\"blanchedalmond\"}\n",
    "ax = sns.boxplot(x=\"difficulty\", y=\"climb\", data=hiking, \n",
    "                 order=[\"1 - easy\", \"2 - medium\", \"3 - difficult\", \"0 - no rating\"],\n",
    "                palette = my_pal) \\\n",
    ".set_title('Relationship between Trail Difficulty Rating and Elevation Gain')\\\n",
    "\n",
    "# https://matplotlib.org/2.0.2/examples/color/named_colors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"difficulty\", y=\"length\",showfliers = False, data=hiking, \n",
    "                 order=[\"1 - easy\", \"2 - medium\", \"3 - difficult\", \"0 - no rating\"],\n",
    "                palette = my_pal) \\\n",
    ".set_title('Relationship between Trail Difficulty Rating and Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Correlation Matrix\n",
    "Let's see what the mathematical correlations are between the four parameters: climb, length, fitness score, technique score. For that, trails without any rating will be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numerical columns that are not needed and remove rows with no rating \n",
    "corr_matrix_df = hiking.drop(columns = ['min','max']).copy()\n",
    "corr_matrix_df = corr_matrix_df[corr_matrix_df['difficulty'] != '0 - no rating']\n",
    "# create a correlation matrix\n",
    "corr_matrix = corr_matrix_df.corr()\n",
    "plt.figure(figsize=(14,8))\n",
    "sns.heatmap(corr_matrix, annot = True, cmap = 'Blues');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Fitness' and 'Technique' have the highest correlation among all measures.\n",
    "What about the correlation with the 'Difficulty Level'?\n",
    "We therefore convert the 'easy', 'medium', 'difficult' in numeric values and build a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add numerical column indicating the difficulty level\n",
    "corr_matrix_df['difficulty_level'] = corr_matrix_df['difficulty']\n",
    "corr_matrix_df.loc[corr_matrix_df['difficulty'] == '1 - easy', ['difficulty_level']] = 1\n",
    "corr_matrix_df.loc[corr_matrix_df['difficulty'] == '2 - medium', ['difficulty_level']] = 2\n",
    "corr_matrix_df.loc[corr_matrix_df['difficulty'] == '3 - difficult', ['difficulty_level']] = 3\n",
    "\n",
    "# the column 'difficulty_level' will return object type which we need to convert to integers.\n",
    "corr_matrix_df['difficulty_level']= corr_matrix_df['difficulty_level'].astype(str).astype(int)\n",
    "#or corr_matrix_df2 = pd.to_numeric(corr_matrix_df['difficulty_level']).astype(int)\n",
    "corr_matrix_df2 = corr_matrix_df.copy()\n",
    "corr_matrix_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_level = corr_matrix_df2.corr()\n",
    "plt.figure(figsize=(14,8))\n",
    "sns.heatmap(corr_matrix_level, annot = True, cmap = 'Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the highest correlation is between the fitness level and the difficulty level, followed by the technique and the difficulty level. This was not visible in the box plot graphs above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Download Trail GPS Data\n",
    "\n",
    "The next part of this notebook deals with parsing HTML tags again, however this time around we will be gathering the GPS data needed to create the snowfall alert in Milestone 3. The GPS data for each trail is actually a list of GPS points that comprise the length of the trail, from start to summit. However, we will only be using the first GPS point in our algorithm, which is the trail-head. \n",
    "\n",
    "The Bergfex website embeds their GPS data into downloadable files, which are meant to be uploaded to a GPS device for navigation purposes. As such, the following code will automatically download each GPS file into a folder. Importantly, a sleep timer is added to replicate how a human may download these files, if the sleep timer is not added, the website will become suspicious and prevent us from downloading more files. \n",
    "\n",
    "The type of GPS file that is being downloaded is a \"GPX\" or \"GPS Exchange Format\" file, which is a special type of XML file that can be used to describe waypoints. \n",
    "\n",
    "Finally, we crosscheck with Google Earth Web App, that our downloaded GPS data is exactly what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Download Batches of GPX Files\n",
    "\n",
    "This loop uses the activity id's to navigate to each of the activity's pages. It then downloads the GPS file to the folder \"gpx_files\" and saves it as a filename \"Unique_id.gpx\". To prevent being blocked from the page, we download the data in small batches (70) for all activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = corr_matrix_df.iloc[:10,0]\n",
    "test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a small subset of data for testing\n",
    "\"\"\"test_id = corr_matrix_df.iloc[:5,0]\n",
    "for hike_id in test_id: \n",
    "    url_id = f\"https://www.bergfex.com/downloads/gps/?type=&id={str(hike_id)};fileType=kml\"\n",
    "    print(url_id)\n",
    "    r = requests.get(url_id, allow_redirects=True)          # Send the request\n",
    "    time.sleep(5)                                           # Include a timer to prevent blocking\n",
    "    write_link = ('../gpx_files/'+ str(hike_id) + \".gpx\")   # Create gpx file\n",
    "    with open(write_link, 'wb') as f:\n",
    "        f.write(r.content)                                  # Write data for each ID in each file\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the content of one download. There are lots of data points of which the key data needs to be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"r.content\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.2 Extract Coordinates from GPX files\n",
    "\n",
    "Now that the GPX files have been downloaded to a local file, we can loop through them and extract the important information, which is the latitude and the longitude of the trail start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"list_of_trails_list = []\n",
    "list_of_files = glob.glob('*gpx')                     # Create a list for each trail_id\n",
    "for hike_id in test_id: \n",
    "    trail_gps_list = []\n",
    "    gpx = gpxpy.parse(open('../gpx_files/'+str(hike_id)+'.gpx', 'r')) # Read content for each trail_id\n",
    "    trail_gps_list.append(hike_id)\n",
    "    for track in gpx.tracks: \n",
    "        trail_name = track.name                 # Track name\n",
    "        trail_gps_list.append(trail_name)\n",
    "        for segment in track.segments: \n",
    "            for point in segment.points: \n",
    "              lat = point.latitude              # Latitude\n",
    "              trail_gps_list.append(lat)\n",
    "              lon = point.longitude             # Longitude\n",
    "              trail_gps_list.append(lon) \n",
    "              break\n",
    "        list_of_trails_list.append(trail_gps_list)\n",
    "\n",
    "print(list_of_trails_list)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3  Create a Dataframe and save as CSV\n",
    "\n",
    "Let's take a look at our data now that it has been extracted. It looks like everything is working as intended. Each trail has a unique ID and latitude and longitude. This data can now be saved to a CSV file which will be used for the snow alert in the next notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"trail_coords_df = pd.DataFrame(list_of_trails_list)        # save to DF\n",
    "trail_coords_df.columns=['trail_id', 'name', 'lat', 'lon'] # name the columns\n",
    "trail_coords_df.to_csv('../data/trail_coords_df.csv', index=False) # save to CSV\n",
    "coords_df = pd.read_csv('../data/trail_coords_df.csv')\n",
    "coords_df\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Check GPS example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test one ID (48850) with its coordinates (lat = 46.718933, lon = 8.037732) in Google Earth web app. It correctly displays the location. \n",
    "\n",
    "Bergfex GPS data:\n",
    "- https://www.bergfex.com/downloads/gps/?type=&id=48850&fileType=kml\n",
    "\n",
    "Google Earth web app:\n",
    "- https://earth.google.com/web/search/46%2e718933+8%2e037732/@46.72096543,8.03751163,1503.22852736a,741.78182338d,35y,-80.11801825h,45.00028446t,0r/data=ClYaLBImGfGBHf8FXEdAIX0G1JtREyBAKhI0Ni43MTg5MzMgOC4wMzc3MzIYAiABIiYKJAkoo5mexUVHQBGyfRYeu0FHQBkErEmOIJsdQCGM9eFheUIdQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to delete:\n",
    "# https://www.bergfex.com/downloads/gps/?type=&id=131699&fileType=kml\n",
    "# https://www.bergfex.com/downloads/gps/?type=&id=48850&fileType=kml\n",
    "#Let's test one ID (131699) with its coordinates (lat = 46.508190, lon = 7.358150)\n",
    "#Let's test one ID (48850) with its coordinates (lat = 46.718933, lon = 8.037732)\n",
    "#48850\n",
    "#46.718933\n",
    "#8.037732\n",
    "#https://earth.google.com/web/search/46%2e508190+7%2e358150/@46.50819,7.35815,2006.43177272a,699.98943356d,35y,0h,45t,0r/data=ClYaLBImGb4Ts14MQUdAIeU_pN--bh1AKhI0Ni41MDgxOTAgNy4zNTgxNTAYAiABIiYKJAlWqScNN6A1QBFWqScNN6A1wBkg4Hxc5e7VPyFlaK6dDdtZwA\n",
    "#https://earth.google.com/web/search/46%2e718933+8%2e037732/@46.72096543,8.03751163,1503.22852736a,741.78182338d,35y,-80.11801825h,45.00028446t,0r/data=ClYaLBImGfGBHf8FXEdAIX0G1JtREyBAKhI0Ni43MTg5MzMgOC4wMzc3MzIYAiABIiYKJAkoo5mexUVHQBGyfRYeu0FHQBkErEmOIJsdQCGM9eFheUIdQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"example = coords_df[coords_df['trail_id'] == 48850]\n",
    "example\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# needed to increase the display size of the png\n",
    "%matplotlib notebook \n",
    "img = mpimg.imread('../examples/GoogleEarth.jpeg')\n",
    "plt.rcParams['figure.dpi'] = 140\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Functions for GPS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we confirmed the GPS data, let's combine the code in functions. By adding tranches and a timer for their download, we make sure we will not be blocked by the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to download the GPX data by regions and in tranches\n",
    "We recommend a maximum tranch of 62 ID's with a timer of 2000 seconds in between. Executing this for all ID's can take quite some time, so we recommend to execute it by region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpx_data(region, tranch_len, timer, save_as_csv = True):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for_gpx_df = pd.read_csv(f\"../data/activities_{region}.csv\") # import csv file for resp. region\n",
    "    n_tranches = math.ceil(len(for_gpx_df.iloc[:,0])/ tranch_len) # calculate # of tranches\n",
    "    \n",
    "    tranch_df = pd.DataFrame()\n",
    "    list_of_trails_list = []\n",
    "    start_t = 0\n",
    "    end_t = tranch_len\n",
    "    \n",
    "    for i in range(n_tranches): # subset all IDs into different tranches \n",
    "        tranch_df = for_gpx_df.iloc[start_t:end_t,0]\n",
    "        start_t = start_t + tranch_len                # set new subset start\n",
    "        end_t = start_t + tranch_len                  # set new subset end\n",
    "        tranch_df = tranch_df.fillna(0).astype(int)   # format df\n",
    "        ids_df = tranch_df.loc[(tranch_df != 0)]      # filter out non-valid IDs\n",
    "        \n",
    "        # download file for each ID of the subset\n",
    "        for ids in ids_df: \n",
    "            url_id = f\"https://www.bergfex.com/downloads/gps/?type=&id={str(ids)};fileType=kml\"\n",
    "            r = requests.get(url_id, allow_redirects=True)    # Send the request\n",
    "            write_link = ('../gpx_files/'+ str(ids) + \".gpx\") # Create gpx file\n",
    "            with open(write_link, 'wb') as f:\n",
    "                f.write(r.content)                            # Write data for each ID in each file\n",
    "            # extract the trail_id, name of the hike, lat, lon from GPX files\n",
    "            trail_gps_list = []\n",
    "            gpx = gpxpy.parse(open('../gpx_files/'+str(ids)+'.gpx', 'r')) # Read content for each trail_id\n",
    "            trail_gps_list.append(ids)                  # ID\n",
    "            for track in gpx.tracks: \n",
    "                trail_name = track.name                 # Track name\n",
    "                trail_gps_list.append(trail_name)\n",
    "                for segment in track.segments: \n",
    "                    for point in segment.points: \n",
    "                      lat = point.latitude              # Latitude\n",
    "                      trail_gps_list.append(lat)\n",
    "                      lon = point.longitude             # Longitude\n",
    "                      trail_gps_list.append(lon) \n",
    "                      break\n",
    "                list_of_trails_list.append(trail_gps_list)\n",
    "        time.sleep(timer) # timer to prevent being blocked\n",
    "    \n",
    "    trail_coords_df = pd.DataFrame(list_of_trails_list)        # save to DF\n",
    "    trail_coords_df.columns=['trail_id', 'name', 'lat', 'lon'] # name the columns\n",
    "    if save_as_csv == True:                                    # Option to save to csv\n",
    "        trail_coords_df.to_csv(f'../data/trail_coords_df_{region}.csv', index=False)\n",
    "    print(\"-- {0} seconds -- for {1} tranches\".format(time.time() - start_time, n_tranches)) # time needed\n",
    "    \n",
    "    return trail_coords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to add GPS data from downloaded GPX files to activity DF\n",
    "Once we have all the GPX files for one region, we can add them to the activity df by merging the tables on the ID and removing duplicate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_activities_gpx(region):\n",
    "    act_df = pd.read_csv(f\"../data/activities_{region}.csv\")\n",
    "    gpx_df = pd.read_csv(f\"../data/trail_coords_df_{region}.csv\")\n",
    "    act_gpx = act_df.merge(gpx_df, how=\"left\", left_on = 'ID', right_on = \"trail_id\")\n",
    "    act_gpx = act_gpx.drop(columns=['name', 'trail_id'])\n",
    "    act_gpx.to_csv(f\"../data/activities_{region}_gpx.csv\", index=False)\n",
    "    return act_gpx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test GPS functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if above functions work. For that we define the name of the region, set the tranch length to '2' and the timer to '2' seconds. We select 5 activities from the 'bern-region' DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reg_special = 'test_reg'\n",
    "tranch_len = 2\n",
    "timer = 2\n",
    "\n",
    "test_ids = activities_df_br.iloc[25:30,:]\n",
    "test_ids.to_csv(f'../data/activities_{test_reg_special}.csv', index=False)\n",
    "result_test = get_gpx_data(test_reg_special, tranch_len, timer, save_as_csv = True)\n",
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test2 = merge_activities_gpx(test_reg_special)\n",
    "result_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTINUE IN NOTEBOOK II"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bergfex_I",
   "language": "python",
   "name": "bergfex_i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
